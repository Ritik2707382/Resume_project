{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZvXda99JUUu"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        document = fitz.open(file_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(document.page_count):\n",
        "            page = document.load_page(page_num)\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    elif file_path.endswith('.docx'):\n",
        "        doc = Document(file_path)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + '\\n'\n",
        "        return text\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "def extract_name(text):\n",
        "    # Look for typical name patterns, including fully uppercase names\n",
        "    match = re.search(r'^(Name:\\s*)?([A-Z][a-z]+ [A-Z][a-z]+|[A-Z ]{2,})$', text, re.MULTILINE)\n",
        "    return match.group(2).strip().title() if match else \"Name not found\"\n",
        "\n",
        "def extract_email(text):\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    match = re.search(email_pattern, text)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "\n",
        "def extract_phone(text):\n",
        "    phone_pattern = r'(\\+?\\d{1,3}[\\s-]?)?(\\(?\\d{3}\\)?[\\s-]?\\d{3}[\\s-]?\\d{4})'\n",
        "    match = re.search(phone_pattern, text)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "\n",
        "def extract_work_experience(text):\n",
        "    work_experience = re.search(r'WORK EXPERIENCE\\s*(.*?)(?:EDUCATION|SKILLS|PROJECTS|CONTACT|$)', text,\n",
        "                                re.DOTALL | re.IGNORECASE)\n",
        "    if work_experience:\n",
        "        return work_experience.group(1).strip()\n",
        "    return ''\n",
        "\n",
        "def extract_skills(text):\n",
        "    # Remove sections like 'CAREER OBJECTIVE' and 'CERTIFICATIONS' entirely\n",
        "    cleaned_text = re.sub(r'CAREER OBJECTIVE.*?(?=(WORK EXPERIENCE|EDUCATION|SKILLS|PROJECTS|CONTACT|$))', '', text,\n",
        "                          flags=re.DOTALL | re.IGNORECASE)\n",
        "    cleaned_text = re.sub(r'CERTIFICATIONS.*?(?=(WORK EXPERIENCE|EDUCATION|SKILLS|PROJECTS|CONTACT|$))', '',\n",
        "                          cleaned_text, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    # Remove any remaining lines that contain career objectives or irrelevant text\n",
        "    cleaned_text = re.sub(r'(^.*Seeking.*$|^.*skills and outstanding aptitude for learning.*$|^.*Grumman.*$)', '',\n",
        "                          cleaned_text, flags=re.MULTILINE | re.IGNORECASE)\n",
        "\n",
        "    skills_section = re.search(r'SKILLS\\s*(.*?)(?:WORK EXPERIENCE|EDUCATION|PROJECTS|CONTACT|$)', cleaned_text,\n",
        "                               re.DOTALL | re.IGNORECASE)\n",
        "    if skills_section:\n",
        "        skills_text = skills_section.group(1).strip()\n",
        "        # Split the skills by newlines or commas\n",
        "        skills_list = re.split(r'\\n|,', skills_text)\n",
        "        # Filter out any empty strings and unnecessary entries\n",
        "        filtered_skills = [skill.strip() for skill in skills_list if skill.strip()]\n",
        "        # Remove duplicates while preserving order\n",
        "        unique_skills = []\n",
        "        for skill in filtered_skills:\n",
        "            if skill not in unique_skills:\n",
        "                unique_skills.append(skill)\n",
        "        return unique_skills\n",
        "    return []\n",
        "\n",
        "def extract_projects(text):\n",
        "    projects_section = re.search(r'PROJECTS\\s*(.*?)(?:WORK EXPERIENCE|EDUCATION|SKILLS|CONTACT|$)', text,\n",
        "                                 re.DOTALL | re.IGNORECASE)\n",
        "    if projects_section:\n",
        "        projects_text = projects_section.group(1)\n",
        "        projects = re.split(r'\\n', projects_text)\n",
        "        return [project.strip() for project in projects if project.strip()]\n",
        "    return []\n",
        "\n",
        "\n",
        "def score_resume(work_experience, skills, projects, required_skills):\n",
        "    score = 0\n",
        "    score_details = {\n",
        "        'experience': 0,\n",
        "        'skills': 0,\n",
        "        'projects': 0\n",
        "    }\n",
        "\n",
        "    # Assuming each year of experience counts as 1 point\n",
        "    experience_years = re.findall(r'\\b(\\d+)\\s+years?', work_experience, re.IGNORECASE)\n",
        "    for exp in experience_years:\n",
        "        score_details['experience'] += int(exp)\n",
        "    score += score_details['experience']\n",
        "\n",
        "    # Each matching skill adds 2 points\n",
        "    for skill in skills:\n",
        "        if skill.lower() in [req_skill.lower() for req_skill in required_skills]:\n",
        "            score_details['skills'] += 2\n",
        "    score += score_details['skills']\n",
        "\n",
        "    # Each project adds 1 point\n",
        "    score_details['projects'] = len(projects)\n",
        "    score += score_details['projects']\n",
        "\n",
        "    return score, score_details\n",
        "\n",
        "\n",
        "def parse_job_description(job_description):\n",
        "    skills_section = re.search(r'Skills Required:\\s*(.*?)\\s*Job Description:', job_description,\n",
        "                               re.DOTALL | re.IGNORECASE)\n",
        "    if skills_section:\n",
        "        skills_text = skills_section.group(1)\n",
        "        skills = re.split(r'\\n|,', skills_text)\n",
        "        return [skill.strip() for skill in skills if skill.strip()]\n",
        "    return []\n",
        "\n",
        "\n",
        "def process_resumes(resume_paths, job_description):\n",
        "    required_skills = parse_job_description(job_description)\n",
        "    print(\"Required Skills:\", required_skills)\n",
        "\n",
        "    resume_details = []\n",
        "    for path in resume_paths:\n",
        "        print(f\"\\nProcessing {path}...\")\n",
        "        text = extract_text(path)\n",
        "\n",
        "        name = extract_name(text)\n",
        "        email = extract_email(text)\n",
        "        phone = extract_phone(text)\n",
        "        work_experience = extract_work_experience(text)\n",
        "        skills = extract_skills(text)\n",
        "        projects = extract_projects(text)\n",
        "\n",
        "        print(f\"Extracted Name: {name}\")\n",
        "        print(f\"Extracted Email: {email}\")\n",
        "        print(f\"Extracted Phone: {phone}\")\n",
        "        print(f\"Extracted Work Experience: {work_experience}\")\n",
        "        print(f\"Extracted Skills: {skills}\")\n",
        "        print(f\"Extracted Projects: {projects}\")\n",
        "\n",
        "        score, score_details = score_resume(work_experience, skills, projects, required_skills)\n",
        "\n",
        "        resume_details.append({\n",
        "            'name': name,\n",
        "            'email': email,\n",
        "            'phone': phone,\n",
        "            'work_experience': work_experience,\n",
        "            'skills': skills,\n",
        "            'projects': projects,\n",
        "            'score': score,\n",
        "            'score_details': score_details\n",
        "        })\n",
        "\n",
        "    sorted_resumes = sorted(resume_details, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return sorted_resumes\n",
        "\n",
        "\n",
        "# Example usage\n",
        "resume_paths = ['/content/resume3.pdf', '/content/resume1.pdf', '/content/resume2.docx']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nName: {resume['name']}\")\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"\\nPhone: {resume['phone']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTcSEIqGKJgC",
        "outputId": "e0f45af0-4de0-4057-e927-a1bbb62c7cd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume3.pdf...\n",
            "Extracted Name: Kandace Loudor\n",
            "Extracted Email: kloudor@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Scientist\n",
            "Grubhub\n",
            "June 2018 - current / Princeton, NJ\n",
            "Deployed a recommendation engine to production to\n",
            "conditionally recommend other menu items based on past order\n",
            "history, increasing average order size by 7%\n",
            "Implemented various time series forecasting techniques to\n",
            "predict surge in orders, lowering customer wait by 10 minutes\n",
            "Designed a model in a pilot to increase incentives for drivers\n",
            "during peak hours, increasing driver availability by 22%\n",
            "Led a team of 3 data scientist to model the ordering process 5\n",
            "unique ways, reported results, and made recommendations to\n",
            "increase order output by 9%\n",
            "Data Scientist\n",
            "Spectrix Analytical Services\n",
            "March 2016 - June 2018 / Princeton, NJ\n",
            "Built a customer attrition random forest model that improved\n",
            "monthly retention by 12 basis points for clients likely to opt-out\n",
            "by providing relevant product features for them\n",
            "Coordinated with the product and marketing teams to determine\n",
            "what kind of client interactions resulted in maximized service\n",
            "opt-ins, increasing conversions by 18%\n",
            "Partnered with product team to create a production\n",
            "recommendation engine in Python that improved the length on-\n",
            "page for users with $225K in incremental annual revenue\n",
            "Compiled and analyzed data surrounding the prototypes for a\n",
            "prosthesis, which saved over $1M in its creation\n",
            "Entry-Level Data Analyst\n",
            "Avenica\n",
            "April 2015 - March 2016 / Mount Laurel, NJ\n",
            "Collaborated with product managers to perform cohort analysis\n",
            "that identified an opportunity to reduce pricing by 21% for a\n",
            "segment of users to boost yearly revenue by $560,000\n",
            "Constructed operational reporting in Tableau to improve\n",
            "scheduling contractors, saving $90,000 in the annual budget\n",
            "Implemented a long-term pricing experiment that improved\n",
            "customer lifetime value by 23%\n",
            "Ran, submitted, and reported on monthly client enrollments,\n",
            "services opted in for, and the employees assigned to clients\n",
            "Extracted Skills: ['Python (NumPy', 'Pandas', 'Scikit-learn', 'Keras', 'Flask)', 'SQL (MySQL', 'Postgres)', 'Git', 'Time Series Forecasting', 'Productionizing Models', 'Recommendation Engines', 'Customer Segmentation', 'AWS']\n",
            "Extracted Projects: []\n",
            "\n",
            "Processing /content/resume1.pdf...\n",
            "Extracted Name: Tyler Russell\n",
            "Extracted Email: t.russell@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Entry Clerk\n",
            "Zillow Group\n",
            "2022 - 2023\n",
            "Seattle, WA\n",
            "Generated and maintained an accurate property listing database for 576\n",
            "properties using Excel, reducing data entry errors by 14%\n",
            "Utilized SQL Server to query and retrieve specific property data for\n",
            "analysis, saving the team an average of two hours per week\n",
            "Assisted in data cleaning techniques using Pandas to standardize and\n",
            "normalize property attributes, improving search accuracy by 27%\n",
            "Supported the use of Apache Spark for processing large-scale datasets,\n",
            "reducing data processing time by 54% and enabling real-time analytics on\n",
            "streaming data\n",
            "Extracted Skills: ['Python', 'Jupyter Notebook', 'Pandas', 'Scikit-learn', 'Excel', 'SQL Server', 'AWS', 'Apache Spark', 'skills to contribute to bioinformatics research and support', \"Talus Bio's strategic goals.\"]\n",
            "Extracted Projects: ['Data Hackathon 2022', 'Partipant', '2022', 'Documented the entire model development process in a Jupyter', 'Notebook, providing transparency and reproducibility for future iterations', 'Collaborated with nine team members to integrate AWS S3 for efficient', 'storage and retrieval of large datasets', 'Used Agile project management methodologies to allocate tasks and meet', 'project milestones within the given timeframe', 'Demonstrated strong problem-solving']\n",
            "\n",
            "Processing /content/resume2.docx...\n",
            "Extracted Name: Education\n",
            "Extracted Email: Ambro_@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Scientist Intern\n",
            "County of Ventura\n",
            "  2020 - current\t  Ventura, CA\n",
            "Designed and implemented over 40 machine-learning models for different programs and\n",
            "Extracted Skills: ['Machine and Deep Learning Statistical Analysis Processing Large Data Sets Data Visualization Mathematics', 'Programming Data Wrangling', 'Python']\n",
            "Extracted Projects: ['Veriﬁed results of algorithms to predict future occurrences using real-world programs data with 82% precision Extracted raw data from Twitter APIs and analyzed tweets to generate analysis showing trends in public opinion regarding policy changes', 'Developed a Java application that performed pattern analysis of criminal incidents to help identify and visualize hotspots (vulnerable areas) in the city', 'PROJECTS', 'Image Caption Generator Project in Python', 'Pepperdine - Senior Project', 'Aug 2021 - Dec 2021', 'Designed and created an 2 applications to analyze images and convert to natural language (English) descriptions', 'Utilized deep learning techniques to implement a convolutional neural network (CNN) with recurrent neural network (LSTM) to build the image caption generator', 'Created application in Python using a Keras framework against a Flickr 8K dataset', 'Credit Card Fraud Detection Project', 'Pepperdine - Junior Project', 'Aug 2020 - Jun 2021', 'Created 2 apps that classiﬁed credit card transactions into fraudulent and genuine, ﬁt the models, and plotted performance curves', 'Used R with algorithms such as Decision Trees, Logistic Regression, Artiﬁcial Neural Networks, and Gradient Boosting Classiﬁer', 'Created application in R against 6 credit card transaction databases']\n",
            "\n",
            "Name: Education\n",
            "\n",
            "Email: Ambro_@email.com\n",
            "\n",
            "Phone: (123) 456-7890\n",
            "Work Experience: Data Scientist Intern\n",
            "County of Ventura\n",
            "  2020 - current\t  Ventura, CA\n",
            "Designed and implemented over 40 machine-learning models for different programs and\n",
            "Skills: ['Machine and Deep Learning Statistical Analysis Processing Large Data Sets Data Visualization Mathematics', 'Programming Data Wrangling', 'Python']\n",
            "Projects: ['Veriﬁed results of algorithms to predict future occurrences using real-world programs data with 82% precision Extracted raw data from Twitter APIs and analyzed tweets to generate analysis showing trends in public opinion regarding policy changes', 'Developed a Java application that performed pattern analysis of criminal incidents to help identify and visualize hotspots (vulnerable areas) in the city', 'PROJECTS', 'Image Caption Generator Project in Python', 'Pepperdine - Senior Project', 'Aug 2021 - Dec 2021', 'Designed and created an 2 applications to analyze images and convert to natural language (English) descriptions', 'Utilized deep learning techniques to implement a convolutional neural network (CNN) with recurrent neural network (LSTM) to build the image caption generator', 'Created application in Python using a Keras framework against a Flickr 8K dataset', 'Credit Card Fraud Detection Project', 'Pepperdine - Junior Project', 'Aug 2020 - Jun 2021', 'Created 2 apps that classiﬁed credit card transactions into fraudulent and genuine, ﬁt the models, and plotted performance curves', 'Used R with algorithms such as Decision Trees, Logistic Regression, Artiﬁcial Neural Networks, and Gradient Boosting Classiﬁer', 'Created application in R against 6 credit card transaction databases']\n",
            "Score: 17\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 2 points\n",
            " Projects: 15 points\n",
            " Total: 17 points\n",
            "\n",
            "Name: Tyler Russell\n",
            "\n",
            "Email: t.russell@email.com\n",
            "\n",
            "Phone: (123) 456-7890\n",
            "Work Experience: Data Entry Clerk\n",
            "Zillow Group\n",
            "2022 - 2023\n",
            "Seattle, WA\n",
            "Generated and maintained an accurate property listing database for 576\n",
            "properties using Excel, reducing data entry errors by 14%\n",
            "Utilized SQL Server to query and retrieve specific property data for\n",
            "analysis, saving the team an average of two hours per week\n",
            "Assisted in data cleaning techniques using Pandas to standardize and\n",
            "normalize property attributes, improving search accuracy by 27%\n",
            "Supported the use of Apache Spark for processing large-scale datasets,\n",
            "reducing data processing time by 54% and enabling real-time analytics on\n",
            "streaming data\n",
            "Skills: ['Python', 'Jupyter Notebook', 'Pandas', 'Scikit-learn', 'Excel', 'SQL Server', 'AWS', 'Apache Spark', 'skills to contribute to bioinformatics research and support', \"Talus Bio's strategic goals.\"]\n",
            "Projects: ['Data Hackathon 2022', 'Partipant', '2022', 'Documented the entire model development process in a Jupyter', 'Notebook, providing transparency and reproducibility for future iterations', 'Collaborated with nine team members to integrate AWS S3 for efficient', 'storage and retrieval of large datasets', 'Used Agile project management methodologies to allocate tasks and meet', 'project milestones within the given timeframe', 'Demonstrated strong problem-solving']\n",
            "Score: 12\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 2 points\n",
            " Projects: 10 points\n",
            " Total: 12 points\n",
            "\n",
            "Name: Kandace Loudor\n",
            "\n",
            "Email: kloudor@email.com\n",
            "\n",
            "Phone: (123) 456-7890\n",
            "Work Experience: Data Scientist\n",
            "Grubhub\n",
            "June 2018 - current / Princeton, NJ\n",
            "Deployed a recommendation engine to production to\n",
            "conditionally recommend other menu items based on past order\n",
            "history, increasing average order size by 7%\n",
            "Implemented various time series forecasting techniques to\n",
            "predict surge in orders, lowering customer wait by 10 minutes\n",
            "Designed a model in a pilot to increase incentives for drivers\n",
            "during peak hours, increasing driver availability by 22%\n",
            "Led a team of 3 data scientist to model the ordering process 5\n",
            "unique ways, reported results, and made recommendations to\n",
            "increase order output by 9%\n",
            "Data Scientist\n",
            "Spectrix Analytical Services\n",
            "March 2016 - June 2018 / Princeton, NJ\n",
            "Built a customer attrition random forest model that improved\n",
            "monthly retention by 12 basis points for clients likely to opt-out\n",
            "by providing relevant product features for them\n",
            "Coordinated with the product and marketing teams to determine\n",
            "what kind of client interactions resulted in maximized service\n",
            "opt-ins, increasing conversions by 18%\n",
            "Partnered with product team to create a production\n",
            "recommendation engine in Python that improved the length on-\n",
            "page for users with $225K in incremental annual revenue\n",
            "Compiled and analyzed data surrounding the prototypes for a\n",
            "prosthesis, which saved over $1M in its creation\n",
            "Entry-Level Data Analyst\n",
            "Avenica\n",
            "April 2015 - March 2016 / Mount Laurel, NJ\n",
            "Collaborated with product managers to perform cohort analysis\n",
            "that identified an opportunity to reduce pricing by 21% for a\n",
            "segment of users to boost yearly revenue by $560,000\n",
            "Constructed operational reporting in Tableau to improve\n",
            "scheduling contractors, saving $90,000 in the annual budget\n",
            "Implemented a long-term pricing experiment that improved\n",
            "customer lifetime value by 23%\n",
            "Ran, submitted, and reported on monthly client enrollments,\n",
            "services opted in for, and the employees assigned to clients\n",
            "Skills: ['Python (NumPy', 'Pandas', 'Scikit-learn', 'Keras', 'Flask)', 'SQL (MySQL', 'Postgres)', 'Git', 'Time Series Forecasting', 'Productionizing Models', 'Recommendation Engines', 'Customer Segmentation', 'AWS']\n",
            "Projects: []\n",
            "Score: 0\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 0 points\n",
            " Total: 0 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume2.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-s6xhqjK3VZ",
        "outputId": "607ba4e1-97e9-450f-99dd-e92623eeec91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume2.pdf...\n",
            "Extracted Name: Ambrose\n",
            "Extracted Email: Ambro_@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Scientist Intern\n",
            "County of Ventura\n",
            "2020 - current\n",
            "Ventura, CA\n",
            "Designed and implemented over 40 machine-learning\n",
            "models for different programs and\n",
            "Extracted Skills: ['Machine and Deep Learning', 'Statistical Analysis', 'Processing Large Data Sets', 'Data Visualization', 'Mathematics', 'Programming', 'Data Wrangling', 'skills and outstanding aptitude for']\n",
            "Extracted Projects: ['Verified results of algorithms to predict future occurrences', 'using real-world programs data with 82% precision', 'Extracted raw data from Twitter APIs and analyzed tweets to', 'generate analysis showing trends in public opinion', 'regarding policy changes', 'Developed a Java application that performed pattern', 'analysis of criminal incidents to help identify and visualize', 'hotspots (vulnerable areas) in the city', 'PROJECTS', 'Image Caption Generator Project in Python', 'Pepperdine - Senior Project', 'Aug 2021 - Dec 2021', 'Designed and created an 2 applications to analyze images', 'and convert to natural language (English) descriptions', 'Utilized deep learning techniques to implement a', 'convolutional neural network (CNN) with recurrent neural', 'network (LSTM) to build the image caption generator', 'Created application in Python using a Keras framework', 'against a Flickr 8K dataset', 'Credit Card Fraud Detection Project', 'Pepperdine - Junior Project', 'Aug 2020 - Jun 2021', 'Created 2 apps that\\xa0classified credit card transactions into', 'fraudulent and genuine, fit the models, and plotted', 'performance curves', 'Used R with algorithms such as Decision Trees, Logistic', 'Regression, Artificial Neural Networks, and Gradient', 'Boosting Classifier', 'Created application in R against 6 credit card transaction', 'databases']\n",
            "\n",
            "Email: Ambro_@email.com\n",
            "Work Experience: Data Scientist Intern\n",
            "County of Ventura\n",
            "2020 - current\n",
            "Ventura, CA\n",
            "Designed and implemented over 40 machine-learning\n",
            "models for different programs and\n",
            "Skills: ['Machine and Deep Learning', 'Statistical Analysis', 'Processing Large Data Sets', 'Data Visualization', 'Mathematics', 'Programming', 'Data Wrangling', 'skills and outstanding aptitude for']\n",
            "Projects: ['Verified results of algorithms to predict future occurrences', 'using real-world programs data with 82% precision', 'Extracted raw data from Twitter APIs and analyzed tweets to', 'generate analysis showing trends in public opinion', 'regarding policy changes', 'Developed a Java application that performed pattern', 'analysis of criminal incidents to help identify and visualize', 'hotspots (vulnerable areas) in the city', 'PROJECTS', 'Image Caption Generator Project in Python', 'Pepperdine - Senior Project', 'Aug 2021 - Dec 2021', 'Designed and created an 2 applications to analyze images', 'and convert to natural language (English) descriptions', 'Utilized deep learning techniques to implement a', 'convolutional neural network (CNN) with recurrent neural', 'network (LSTM) to build the image caption generator', 'Created application in Python using a Keras framework', 'against a Flickr 8K dataset', 'Credit Card Fraud Detection Project', 'Pepperdine - Junior Project', 'Aug 2020 - Jun 2021', 'Created 2 apps that\\xa0classified credit card transactions into', 'fraudulent and genuine, fit the models, and plotted', 'performance curves', 'Used R with algorithms such as Decision Trees, Logistic', 'Regression, Artificial Neural Networks, and Gradient', 'Boosting Classifier', 'Created application in R against 6 credit card transaction', 'databases']\n",
            "Score: 30\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 30 points\n",
            " Total: 30 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume4.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ohqh_B6LGqH",
        "outputId": "45f13db6-02d0-428b-c9db-17cf49a1c6d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume4.pdf...\n",
            "Extracted Name: Trish Mathers\n",
            "Extracted Email: tmathers@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Niantic\n",
            "Data Scientist Intern\n",
            "Seattle, WA | April 2022 - December 2022\n",
            "Developed a program in SAS that automated refinement of linear\n",
            "regression models for specific segments of a customer base that\n",
            "saved 22 hours of labor per month.\n",
            "Received, cleaned, and prepped data from client using SAS, SQL, and\n",
            "Excel to help data scientists build marketing mix models that resulted\n",
            "in a lift in ROI of 10 basis points.\n",
            "Seattle University Tutor Center\n",
            "Statistics and Mathematics Tutor\n",
            "Seattle, WA | April 2020 - April 2022\n",
            "Assessed students' learning to determine learning weaknesses and\n",
            "needs, successfully helping students perform 13% better in algebra,\n",
            "pre-calculus, calculus, and statistics undergraduate courses.\n",
            "Met with 30+ students per week through online learning platforms or\n",
            "in a 1:1 setting at the tutor center.\n",
            "Scheduled weekly appointments for students, and set schedules for\n",
            "student statistics and math tutors.\n",
            "Communicated with professors about curriculum, and submitted\n",
            "reports 2 times per week to maintain up-to-date learning plans for\n",
            "students.\n",
            "Extracted Skills: ['Programming: SAS (base', 'SAS and Macros)', 'SQL', 'Supervised Learning:', 'linear and logistic', 'regressions', 'decision', 'trees', 'support vector', 'machines (SVM)', 'Unsupervised Learning: k-', 'means clustering', 'principal component', 'analysis (PCA)', 'Data Visualization: Excel', 'Google Sheets']\n",
            "Extracted Projects: ['Fantasy Football Models', 'Aggregated and prepped 3 years of fantasy football projection data', 'from 3 independent sources into a MySQL database.', 'Created a random forest model in SAS, combining disparate sources', 'into one projection that outperformed the mean absolute error of', 'the next best projection by 15%.', 'Entertainment Engine', 'Aggregated data from IMDB and Rotten Tomatoes, and used k-', 'nearest-neighbors in SAS, constructing an enhanced entertainment', 'selection targeted to reach 15- to 25-year-olds.', 'Improved methodologies to save an average of 12 minutes per', 'movie selection and 3 minutes per song selection.']\n",
            "\n",
            "Email: tmathers@email.com\n",
            "Work Experience: Niantic\n",
            "Data Scientist Intern\n",
            "Seattle, WA | April 2022 - December 2022\n",
            "Developed a program in SAS that automated refinement of linear\n",
            "regression models for specific segments of a customer base that\n",
            "saved 22 hours of labor per month.\n",
            "Received, cleaned, and prepped data from client using SAS, SQL, and\n",
            "Excel to help data scientists build marketing mix models that resulted\n",
            "in a lift in ROI of 10 basis points.\n",
            "Seattle University Tutor Center\n",
            "Statistics and Mathematics Tutor\n",
            "Seattle, WA | April 2020 - April 2022\n",
            "Assessed students' learning to determine learning weaknesses and\n",
            "needs, successfully helping students perform 13% better in algebra,\n",
            "pre-calculus, calculus, and statistics undergraduate courses.\n",
            "Met with 30+ students per week through online learning platforms or\n",
            "in a 1:1 setting at the tutor center.\n",
            "Scheduled weekly appointments for students, and set schedules for\n",
            "student statistics and math tutors.\n",
            "Communicated with professors about curriculum, and submitted\n",
            "reports 2 times per week to maintain up-to-date learning plans for\n",
            "students.\n",
            "Skills: ['Programming: SAS (base', 'SAS and Macros)', 'SQL', 'Supervised Learning:', 'linear and logistic', 'regressions', 'decision', 'trees', 'support vector', 'machines (SVM)', 'Unsupervised Learning: k-', 'means clustering', 'principal component', 'analysis (PCA)', 'Data Visualization: Excel', 'Google Sheets']\n",
            "Projects: ['Fantasy Football Models', 'Aggregated and prepped 3 years of fantasy football projection data', 'from 3 independent sources into a MySQL database.', 'Created a random forest model in SAS, combining disparate sources', 'into one projection that outperformed the mean absolute error of', 'the next best projection by 15%.', 'Entertainment Engine', 'Aggregated data from IMDB and Rotten Tomatoes, and used k-', 'nearest-neighbors in SAS, constructing an enhanced entertainment', 'selection targeted to reach 15- to 25-year-olds.', 'Improved methodologies to save an average of 12 minutes per', 'movie selection and 3 minutes per song selection.']\n",
            "Score: 12\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 12 points\n",
            " Total: 12 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume5.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WKf1cyOLvL-",
        "outputId": "2c479429-6fca-4c2d-dbf9-9d6e69152e00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume5.pdf...\n",
            "Extracted Name: Work Experience\n",
            "Extracted Email: tcoleman@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Best Buy - Senior Data Scientist\n",
            "October 2018 - current\n",
            "Remote\n",
            "· Led data extraction and evaluation efforts to save Best Buy more than 11M over the course of tenure\n",
            "· Partnered with product team to build a production recommendation engine in Python that improved the\n",
            "average length on page for users and resulted in $450K in incremental annual revenue\n",
            "· Created a customer attrition random forest model, improving monthly retention by 6 basis points for\n",
            "customers likely to attrit by servicing relevant product features for them\n",
            "· Communicated with PMs to lead 4 data scientists in project planning, development, and execution\n",
            "· Coached data team throughout short and long-term\n",
            "Extracted Skills: ['Python (NumPy', 'Pandas', 'Scikit-learn', 'Flask)', 'SAS; SQL - Redshift', 'MySQL; ElasticSearch; Recommendation', 'Engines', 'Customer Segmentation & Retention Models', 'Price Optimization', 'Productionizing Models']\n",
            "Extracted Projects: [', redefining documentation frequently', '2U - Data Scientist', 'April 2014 - October 2018', 'Brooklyn, NY', '· Conducted A/B testing to solve client pain points in learning platforms, and identified and recommended', 'solutions to solve unclear platform roadmaps, which reduced the bounce rate by 62%', '· Extracted data from 7 disparate sources, and increased agility and accuracy with a centralized system', '· Constructed decisions trees to optimize needed algorithms to better target the learning audience by 15%', '2U - Data Analyst', 'April 2012 - April 2014', 'Brooklyn, NY', '· Determined, using Python clustering methods, groups of states where underwriting models were', 'underperforming, and owned improvements to increase profit by 4%', '· Identified procedural areas of improvement through customer data to help improve the profitability of a', 'nationwide retention program by 8%', '· Developed and owned the reporting for a nationwide retention program using Python, SQL, and Excel,', 'saving an average of 60 hours of labor each month']\n",
            "\n",
            "Email: tcoleman@email.com\n",
            "Work Experience: Best Buy - Senior Data Scientist\n",
            "October 2018 - current\n",
            "Remote\n",
            "· Led data extraction and evaluation efforts to save Best Buy more than 11M over the course of tenure\n",
            "· Partnered with product team to build a production recommendation engine in Python that improved the\n",
            "average length on page for users and resulted in $450K in incremental annual revenue\n",
            "· Created a customer attrition random forest model, improving monthly retention by 6 basis points for\n",
            "customers likely to attrit by servicing relevant product features for them\n",
            "· Communicated with PMs to lead 4 data scientists in project planning, development, and execution\n",
            "· Coached data team throughout short and long-term\n",
            "Skills: ['Python (NumPy', 'Pandas', 'Scikit-learn', 'Flask)', 'SAS; SQL - Redshift', 'MySQL; ElasticSearch; Recommendation', 'Engines', 'Customer Segmentation & Retention Models', 'Price Optimization', 'Productionizing Models']\n",
            "Projects: [', redefining documentation frequently', '2U - Data Scientist', 'April 2014 - October 2018', 'Brooklyn, NY', '· Conducted A/B testing to solve client pain points in learning platforms, and identified and recommended', 'solutions to solve unclear platform roadmaps, which reduced the bounce rate by 62%', '· Extracted data from 7 disparate sources, and increased agility and accuracy with a centralized system', '· Constructed decisions trees to optimize needed algorithms to better target the learning audience by 15%', '2U - Data Analyst', 'April 2012 - April 2014', 'Brooklyn, NY', '· Determined, using Python clustering methods, groups of states where underwriting models were', 'underperforming, and owned improvements to increase profit by 4%', '· Identified procedural areas of improvement through customer data to help improve the profitability of a', 'nationwide retention program by 8%', '· Developed and owned the reporting for a nationwide retention program using Python, SQL, and Excel,', 'saving an average of 60 hours of labor each month']\n",
            "Score: 17\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 17 points\n",
            " Total: 17 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume6.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEtlIhJgMLdN",
        "outputId": "9a2608de-45e7-4a3f-9c05-485d3dd44b7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume6.pdf...\n",
            "Extracted Name: Yasmin Patel\n",
            "Extracted Email: y.patel@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Retail associate\n",
            "TJ Maxx\n",
            "2022 - current\n",
            "Cambridge, MA\n",
            "Exceeded monthly sales targets by 22%, contributing to the\n",
            "store's recognition as a top-performing location.\n",
            "Updated store layouts to increase customer engagement with\n",
            "featured products by 48%.\n",
            "Recognized by management for providing exceptional service\n",
            "after earning an average customer satisfaction rating of 93%.\n",
            "Conducted regular stock checks using inventory management\n",
            "systems, which minimized out-of-stock incidents by 29%.\n",
            "Extracted Skills: ['NumPy', 'Scikit-learn', 'dplyr', 'MySQL', 'SQLite', 'Keras']\n",
            "Extracted Projects: ['Library assistant', 'Harvard University', '2022', 'Recommended personalized book titles to library patrons that', 'led to 89% satisfaction ratings.', 'Collaborated with local nonprofits to host literacy initiatives,', 'growing participation by 28% per month.', 'Designed captivating book displays that boosted checkouts in', 'promoted genres by 47%.', 'Developed a book tracking system with SQLite to improve', 'cataloging accuracy, reducing data entry errors by 31%.', 'Event staff', 'Harvard University', '2021', 'Decorated venues for 42 campus events, earning an average', 'satisfaction rating of 4.8/5 from attendees.', 'Engaged event attendees by actively participating in crowd', 'activities, which boosted guest participation by 38%.', 'Scanned 1100+ tickets per event to maintain an average wait', 'time of under 30 seconds for guests at entry points.', 'Provided exceptional guest assistance by resolving 92% of', 'inquiries on the spot.', 'OBJECTIVE', 'With a strong academic background in', 'computer science from Harvard', 'University and eager to kickstart my', 'career as a data scientist intern at', 'IBM. Proficient in NumPy, Scikit-learn,', 'dplyr, MySQL, SQLite, and Keras with', 'hope to learn from industry experts,', 'tackle complex data challenges, and', \"support IBM's production of cutting-\", 'edge technology.']\n",
            "\n",
            "Email: y.patel@email.com\n",
            "Work Experience: Retail associate\n",
            "TJ Maxx\n",
            "2022 - current\n",
            "Cambridge, MA\n",
            "Exceeded monthly sales targets by 22%, contributing to the\n",
            "store's recognition as a top-performing location.\n",
            "Updated store layouts to increase customer engagement with\n",
            "featured products by 48%.\n",
            "Recognized by management for providing exceptional service\n",
            "after earning an average customer satisfaction rating of 93%.\n",
            "Conducted regular stock checks using inventory management\n",
            "systems, which minimized out-of-stock incidents by 29%.\n",
            "Skills: ['NumPy', 'Scikit-learn', 'dplyr', 'MySQL', 'SQLite', 'Keras']\n",
            "Projects: ['Library assistant', 'Harvard University', '2022', 'Recommended personalized book titles to library patrons that', 'led to 89% satisfaction ratings.', 'Collaborated with local nonprofits to host literacy initiatives,', 'growing participation by 28% per month.', 'Designed captivating book displays that boosted checkouts in', 'promoted genres by 47%.', 'Developed a book tracking system with SQLite to improve', 'cataloging accuracy, reducing data entry errors by 31%.', 'Event staff', 'Harvard University', '2021', 'Decorated venues for 42 campus events, earning an average', 'satisfaction rating of 4.8/5 from attendees.', 'Engaged event attendees by actively participating in crowd', 'activities, which boosted guest participation by 38%.', 'Scanned 1100+ tickets per event to maintain an average wait', 'time of under 30 seconds for guests at entry points.', 'Provided exceptional guest assistance by resolving 92% of', 'inquiries on the spot.', 'OBJECTIVE', 'With a strong academic background in', 'computer science from Harvard', 'University and eager to kickstart my', 'career as a data scientist intern at', 'IBM. Proficient in NumPy, Scikit-learn,', 'dplyr, MySQL, SQLite, and Keras with', 'hope to learn from industry experts,', 'tackle complex data challenges, and', \"support IBM's production of cutting-\", 'edge technology.']\n",
            "Score: 33\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 33 points\n",
            " Total: 33 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume7.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRGRjqHOMXpd",
        "outputId": "9f307b47-4014-4ec2-8843-6a15f716a1fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume7.pdf...\n",
            "Extracted Name: Marco\n",
            "Extracted Email: m.rodriguez@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Visualization Specialist\n",
            "Intuit\n",
            "2022 - current\n",
            "San Francisco, CA\n",
            "Led a geospatial analysis in Tableau to uncover under-served\n",
            "markets, creating a strategic expansion plan that saw a 16%\n",
            "growth in new customer acquisition.\n",
            "Devised a Microsoft Power BI tool for real-time monitoring of\n",
            "cloud infrastructure costs at Intuit, shrinking unnecessary\n",
            "expenditures by $19,544.\n",
            "Leveraged Matplotlib with real-time data feeds to track e-\n",
            "commerce transactions, identifying a 13% rise in mobile\n",
            "payments.\n",
            "Initiated a data visualization consistency project,\n",
            "standardizing visuals across 246 reports in Illustrator,\n",
            "slashing discrepancies and errors by 43%.\n",
            "Data Analyst\n",
            "Salesforce\n",
            "2019 - 2022\n",
            "San Francisco, CA\n",
            "Automated Salesforce data extraction, reducing data\n",
            "preparation time by 2 hours, allowing the team to produce\n",
            "reports on time. \n",
            "Predicted a 19% rise in Q4 sales with ArcGIS spatial analysis\n",
            "to forecast sales trends, helping the team optimize stock\n",
            "levels which reduced inventory costs by $13,921.\n",
            "Applied cluster analysis in SPSS to segment customers,\n",
            "which enhanced personalized marketing strategies and\n",
            "grew campaign ROI by 18%.\n",
            "Integrated Unity3D's scripting to automate the process of\n",
            "transforming Salesforce CRM data into 3D models, saving 62\n",
            "hours every quarter in manual data processing.\n",
            "Junior Data Analyst\n",
            "Uber\n",
            "2018 - 2019\n",
            "Mountain View, CA\n",
            "Designed a Shiny application to examine and visualize the\n",
            "ideal routes for Uber drivers, resulting in a 24% decrease in\n",
            "fuel consumption which increased driver profitability.\n",
            "Analyzed Uber ride data points to identify key factors\n",
            "influencing ride-sharing preferences, contributing to a 19%\n",
            "increase in customer satisfaction.\n",
            "Implemented GitHub Actions for automatic data quality\n",
            "checks on every push, cutting down data inconsistencies\n",
            "by 26%.\n",
            "Conducted price elasticity analysis for Uber Eats which\n",
            "helped the company shape menu prices more effectively,\n",
            "boosting average order value by 7%.\n",
            "Extracted Skills: ['Tableau', 'Python', 'Matplotlib', 'Microsoft Power BI', 'ArcGIS', 'Adobe Illustrator', 'SPSS', 'Shiny', 'Unity3D', 'GitHub']\n",
            "Extracted Projects: []\n",
            "\n",
            "Email: m.rodriguez@email.com\n",
            "Work Experience: Data Visualization Specialist\n",
            "Intuit\n",
            "2022 - current\n",
            "San Francisco, CA\n",
            "Led a geospatial analysis in Tableau to uncover under-served\n",
            "markets, creating a strategic expansion plan that saw a 16%\n",
            "growth in new customer acquisition.\n",
            "Devised a Microsoft Power BI tool for real-time monitoring of\n",
            "cloud infrastructure costs at Intuit, shrinking unnecessary\n",
            "expenditures by $19,544.\n",
            "Leveraged Matplotlib with real-time data feeds to track e-\n",
            "commerce transactions, identifying a 13% rise in mobile\n",
            "payments.\n",
            "Initiated a data visualization consistency project,\n",
            "standardizing visuals across 246 reports in Illustrator,\n",
            "slashing discrepancies and errors by 43%.\n",
            "Data Analyst\n",
            "Salesforce\n",
            "2019 - 2022\n",
            "San Francisco, CA\n",
            "Automated Salesforce data extraction, reducing data\n",
            "preparation time by 2 hours, allowing the team to produce\n",
            "reports on time. \n",
            "Predicted a 19% rise in Q4 sales with ArcGIS spatial analysis\n",
            "to forecast sales trends, helping the team optimize stock\n",
            "levels which reduced inventory costs by $13,921.\n",
            "Applied cluster analysis in SPSS to segment customers,\n",
            "which enhanced personalized marketing strategies and\n",
            "grew campaign ROI by 18%.\n",
            "Integrated Unity3D's scripting to automate the process of\n",
            "transforming Salesforce CRM data into 3D models, saving 62\n",
            "hours every quarter in manual data processing.\n",
            "Junior Data Analyst\n",
            "Uber\n",
            "2018 - 2019\n",
            "Mountain View, CA\n",
            "Designed a Shiny application to examine and visualize the\n",
            "ideal routes for Uber drivers, resulting in a 24% decrease in\n",
            "fuel consumption which increased driver profitability.\n",
            "Analyzed Uber ride data points to identify key factors\n",
            "influencing ride-sharing preferences, contributing to a 19%\n",
            "increase in customer satisfaction.\n",
            "Implemented GitHub Actions for automatic data quality\n",
            "checks on every push, cutting down data inconsistencies\n",
            "by 26%.\n",
            "Conducted price elasticity analysis for Uber Eats which\n",
            "helped the company shape menu prices more effectively,\n",
            "boosting average order value by 7%.\n",
            "Skills: ['Tableau', 'Python', 'Matplotlib', 'Microsoft Power BI', 'ArcGIS', 'Adobe Illustrator', 'SPSS', 'Shiny', 'Unity3D', 'GitHub']\n",
            "Projects: []\n",
            "Score: 2\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 2 points\n",
            " Projects: 0 points\n",
            " Total: 2 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume8.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0cO74jwUMleE",
        "outputId": "2053507b-ad48-4673-e4dc-46d4acb91dc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume8.pdf...\n",
            "Extracted Name: Aiden Tan\n",
            "Extracted Email: a.tan@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Healthcare Data Scientist\n",
            "Meritus Health\n",
            "2021 - current\n",
            "Hagerstown, MD\n",
            "Developed a predictive readmission model in Python, which achieved an\n",
            "accuracy rate of 86%, helping Meritus Health proactively identify at-risk\n",
            "patients.\n",
            "Leveraged Apache Hadoop to figure out the complexities in healthcare\n",
            "data and discover hidden patterns and anomalies that led to an 18%\n",
            "reduction in diagnostic errors.\n",
            "Devised a Tableau remote monitoring dashboard for tracking vital signs\n",
            "and patient conditions remotely, reducing the need for in-person visits\n",
            "by 36%.\n",
            "Integrated TensorFlow into Meritus Health's electronic health record\n",
            "(EHR) system to allow predictive analytics for clinical decision support,\n",
            "decreasing medical errors by 14%.\n",
            "Healthcare Data Analyst\n",
            "Maxim Healthcare Services\n",
            "2018 - 2021\n",
            "Columbia, MD\n",
            "Created MySQL database backup and recovery procedures for minimal\n",
            "data loss during system failures, reducing recovery time by 2.7 hours.\n",
            "Applied SAS data mining techniques to healthcare claims data, resulting\n",
            "in a 19% decrease in claims processing errors.\n",
            "Proposed AWS Data Pipeline for real-time data integration, ensuring\n",
            "timely access to critical patient data for clinical teams, minimizing data\n",
            "access delays by 41%.\n",
            "Established Epic Systems with external laboratories, reducing result\n",
            "turnaround time by 26 minutes.\n",
            "Data Analyst Intern\n",
            "T. Rowe Price\n",
            "2017 - 2018\n",
            "Baltimore, MD\n",
            "Implemented a sentiment analysis model using NLTK, accurately\n",
            "predicting market trends with an 87% success rate.\n",
            "Conducted in-depth financial data analysis to identify cost-saving\n",
            "opportunities that cut down operating expenses by $24,653.\n",
            "Built ETL (Extract, Transform, Load) processes in Talend, allowing for the\n",
            "seamless integration of external financial data sources, shrinking\n",
            "manual data entry by 22%.\n",
            "Designed a machine learning model to predict market downturns,\n",
            "reducing client losses by 16%.\n",
            "Extracted Skills: ['Python', 'Tableau', 'TensorFlow', 'Apache Hadoop', 'MySQL', 'SAS', 'AWS', 'Epic Systems', 'NLTK', 'Talend']\n",
            "Extracted Projects: []\n",
            "\n",
            "Email: a.tan@email.com\n",
            "Work Experience: Healthcare Data Scientist\n",
            "Meritus Health\n",
            "2021 - current\n",
            "Hagerstown, MD\n",
            "Developed a predictive readmission model in Python, which achieved an\n",
            "accuracy rate of 86%, helping Meritus Health proactively identify at-risk\n",
            "patients.\n",
            "Leveraged Apache Hadoop to figure out the complexities in healthcare\n",
            "data and discover hidden patterns and anomalies that led to an 18%\n",
            "reduction in diagnostic errors.\n",
            "Devised a Tableau remote monitoring dashboard for tracking vital signs\n",
            "and patient conditions remotely, reducing the need for in-person visits\n",
            "by 36%.\n",
            "Integrated TensorFlow into Meritus Health's electronic health record\n",
            "(EHR) system to allow predictive analytics for clinical decision support,\n",
            "decreasing medical errors by 14%.\n",
            "Healthcare Data Analyst\n",
            "Maxim Healthcare Services\n",
            "2018 - 2021\n",
            "Columbia, MD\n",
            "Created MySQL database backup and recovery procedures for minimal\n",
            "data loss during system failures, reducing recovery time by 2.7 hours.\n",
            "Applied SAS data mining techniques to healthcare claims data, resulting\n",
            "in a 19% decrease in claims processing errors.\n",
            "Proposed AWS Data Pipeline for real-time data integration, ensuring\n",
            "timely access to critical patient data for clinical teams, minimizing data\n",
            "access delays by 41%.\n",
            "Established Epic Systems with external laboratories, reducing result\n",
            "turnaround time by 26 minutes.\n",
            "Data Analyst Intern\n",
            "T. Rowe Price\n",
            "2017 - 2018\n",
            "Baltimore, MD\n",
            "Implemented a sentiment analysis model using NLTK, accurately\n",
            "predicting market trends with an 87% success rate.\n",
            "Conducted in-depth financial data analysis to identify cost-saving\n",
            "opportunities that cut down operating expenses by $24,653.\n",
            "Built ETL (Extract, Transform, Load) processes in Talend, allowing for the\n",
            "seamless integration of external financial data sources, shrinking\n",
            "manual data entry by 22%.\n",
            "Designed a machine learning model to predict market downturns,\n",
            "reducing client losses by 16%.\n",
            "Skills: ['Python', 'Tableau', 'TensorFlow', 'Apache Hadoop', 'MySQL', 'SAS', 'AWS', 'Epic Systems', 'NLTK', 'Talend']\n",
            "Projects: []\n",
            "Score: 2\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 2 points\n",
            " Projects: 0 points\n",
            " Total: 2 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume9.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYwriVeCMqIc",
        "outputId": "7febdf1f-f498-4fa1-8c9d-97c13109dc75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume9.pdf...\n",
            "Extracted Name: Emma Davis\n",
            "Extracted Email: e.davis@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Adobe - Data Scientist\n",
            "2018 - current\n",
            "San Jose, CA\n",
            "Led data analysis initiatives that resulted in a 37% increase in customer retention rates.\n",
            "Developed predictive models using TensorFlow, reducing forecasting errors by 21%.\n",
            "Implemented Apache Hadoop to analyze large-scale datasets, improving data processing speed by\n",
            "33%.\n",
            "Utilized Pandas and Python for data manipulation, resulting in a 2-hour reduction in data cleaning\n",
            "time.\n",
            "Cisco Systems - Junior Data Engineer\n",
            "2015 - 2018\n",
            "San Jose, CA\n",
            "Collaborated with a cross-functional team to develop ETL pipelines, improving data processing\n",
            "efficiency by 26%.\n",
            "Leveraged Amazon Redshift to optimize data warehouse performance, resulting in a 3-hour reduction\n",
            "in query execution times.\n",
            "Automated data ingestion processes using AWS Glue, reducing manual effort by 32%.\n",
            "Conducted sentiment analysis on customer reviews using NLTK, providing valuable insights to the\n",
            "marketing team.\n",
            "eBay - Trainee Data Analyst\n",
            "2012 - 2015\n",
            "San Jose, CA\n",
            "Set up Kafka clusters and integrated data sources, resulting in a 30% improvement in data processing\n",
            "efficiency.\n",
            "Achieved a $4K reduction in infrastructure costs by containerizing data processing components.\n",
            "Spearheaded automated deployment scripts and version control using Git, resulting in a 27% decrease\n",
            "in deployment errors.\n",
            "Used Python and SQL to clean and preprocess data, achieving a data quality improvement of 18%.\n",
            "Extracted Skills: ['Python; Pandas; TensorFlow; Apache Hadoop; Amazon Redshift; AWS; NLTK; Apache Kafka; Git; Docker']\n",
            "Extracted Projects: []\n",
            "\n",
            "Email: e.davis@email.com\n",
            "Work Experience: Adobe - Data Scientist\n",
            "2018 - current\n",
            "San Jose, CA\n",
            "Led data analysis initiatives that resulted in a 37% increase in customer retention rates.\n",
            "Developed predictive models using TensorFlow, reducing forecasting errors by 21%.\n",
            "Implemented Apache Hadoop to analyze large-scale datasets, improving data processing speed by\n",
            "33%.\n",
            "Utilized Pandas and Python for data manipulation, resulting in a 2-hour reduction in data cleaning\n",
            "time.\n",
            "Cisco Systems - Junior Data Engineer\n",
            "2015 - 2018\n",
            "San Jose, CA\n",
            "Collaborated with a cross-functional team to develop ETL pipelines, improving data processing\n",
            "efficiency by 26%.\n",
            "Leveraged Amazon Redshift to optimize data warehouse performance, resulting in a 3-hour reduction\n",
            "in query execution times.\n",
            "Automated data ingestion processes using AWS Glue, reducing manual effort by 32%.\n",
            "Conducted sentiment analysis on customer reviews using NLTK, providing valuable insights to the\n",
            "marketing team.\n",
            "eBay - Trainee Data Analyst\n",
            "2012 - 2015\n",
            "San Jose, CA\n",
            "Set up Kafka clusters and integrated data sources, resulting in a 30% improvement in data processing\n",
            "efficiency.\n",
            "Achieved a $4K reduction in infrastructure costs by containerizing data processing components.\n",
            "Spearheaded automated deployment scripts and version control using Git, resulting in a 27% decrease\n",
            "in deployment errors.\n",
            "Used Python and SQL to clean and preprocess data, achieving a data quality improvement of 18%.\n",
            "Skills: ['Python; Pandas; TensorFlow; Apache Hadoop; Amazon Redshift; AWS; NLTK; Apache Kafka; Git; Docker']\n",
            "Projects: []\n",
            "Score: 0\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 0 points\n",
            " Total: 0 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume10.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwFz2W3mM_4F",
        "outputId": "18e2a939-f0aa-4215-a909-e2409365456c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume10.pdf...\n",
            "Extracted Name: David Robinson\n",
            "Extracted Email: d.robinson@email.com\n",
            "Extracted Phone: (123) 456-7890\n",
            "Extracted Work Experience: Data Scientist - DataRobot\n",
            "2017 - current\n",
            "Boston, MA\n",
            "Developed predictive models using PyTorch that improved accuracy by 11%.\n",
            "Designed interactive dashboards in Tableau to visualize key performance indicators, leading to 23%\n",
            "improvement in decision-making processes.\n",
            "Collaborated with cross-functional teams to define data-driven strategies, resulting in a $253K\n",
            "increase in revenue.\n",
            "Conducted A/B testing using Python and statistical methods, optimizing conversion rate by 18%.\n",
            "Junior Data Scientist - Wayfair\n",
            "2014 - 2017\n",
            "Boston, MA\n",
            "Deployed machine learning models on AWS Lambda, improving response time by 2 hours.\n",
            "Used spaCy for named entity recognition (NER) tasks, achieving an accuracy rate of 94%.\n",
            "Extracted, transformed, and loaded (ETL) large datasets, resulting in a 31% reduction in processing\n",
            "time.\n",
            "Created interactive dashboards using Matplotlib and Seaborn to communicate insights effectively.\n",
            "Research Assistant - Massachusetts General Hospital\n",
            "2012 - 2014\n",
            "Boston, MA\n",
            "Analyzed large datasets using MySQL, improving data retrieval efficiency by 44%.\n",
            "Utilized Auto-Sklearn to automate machine learning model selection, reducing modeling time by 2\n",
            "hours.\n",
            "Implemented version control using SVN, resulting in a 28% reduction in code conflicts.\n",
            "Integrated data-driven insights into project strategies, leading to 31% improvements in project\n",
            "outcomes.\n",
            "Extracted Skills: ['domain expertise', 'and a track record of delivering actionable insights to Procter &', \"Gamble's data-driven initiatives.\"]\n",
            "Extracted Projects: []\n",
            "\n",
            "Email: d.robinson@email.com\n",
            "Work Experience: Data Scientist - DataRobot\n",
            "2017 - current\n",
            "Boston, MA\n",
            "Developed predictive models using PyTorch that improved accuracy by 11%.\n",
            "Designed interactive dashboards in Tableau to visualize key performance indicators, leading to 23%\n",
            "improvement in decision-making processes.\n",
            "Collaborated with cross-functional teams to define data-driven strategies, resulting in a $253K\n",
            "increase in revenue.\n",
            "Conducted A/B testing using Python and statistical methods, optimizing conversion rate by 18%.\n",
            "Junior Data Scientist - Wayfair\n",
            "2014 - 2017\n",
            "Boston, MA\n",
            "Deployed machine learning models on AWS Lambda, improving response time by 2 hours.\n",
            "Used spaCy for named entity recognition (NER) tasks, achieving an accuracy rate of 94%.\n",
            "Extracted, transformed, and loaded (ETL) large datasets, resulting in a 31% reduction in processing\n",
            "time.\n",
            "Created interactive dashboards using Matplotlib and Seaborn to communicate insights effectively.\n",
            "Research Assistant - Massachusetts General Hospital\n",
            "2012 - 2014\n",
            "Boston, MA\n",
            "Analyzed large datasets using MySQL, improving data retrieval efficiency by 44%.\n",
            "Utilized Auto-Sklearn to automate machine learning model selection, reducing modeling time by 2\n",
            "hours.\n",
            "Implemented version control using SVN, resulting in a 28% reduction in code conflicts.\n",
            "Integrated data-driven insights into project strategies, leading to 31% improvements in project\n",
            "outcomes.\n",
            "Skills: ['domain expertise', 'and a track record of delivering actionable insights to Procter &', \"Gamble's data-driven initiatives.\"]\n",
            "Projects: []\n",
            "Score: 0\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 0 points\n",
            " Total: 0 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume11.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATFxe4I-NPHE",
        "outputId": "9f52bfac-bdbf-4410-a57f-533609894ac2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume11.pdf...\n",
            "Extracted Name: John Williams\n",
            "Extracted Email: johnwilliams@gmail.com\n",
            "Extracted Phone: 101-900-6543\n",
            "Extracted Work Experience: 2014-2019\n",
            "California\n",
            "GPA: 8.7\n",
            "Senior Data Scientist\n",
            "Tasks\n",
            "• Developed end-to-end machine learning prototypes and scaled them to run in production environments. Increased \n",
            "eficiency by 23%\n",
            "• Derived actionable insights from massive data sets with minimal support. \n",
            "• Provided input into the collection of new data sources and the refinement of existing onces to improve analysis and \n",
            "model development. \n",
            "Achievements\n",
            "• Applied data mining to analyze procurement processes resulting in savings of $420,000 a year. \n",
            "Machine Learning\n",
            "Data Visualization\n",
            "Data Mining\n",
            "Python\n",
            "Scala\n",
            "NLP\n",
            "Problem Solving\n",
            "Fast Learner\n",
            "Leadership\n",
            "Risk Analyisis\n",
            "Time Management\n",
            "Extracted Skills: ['ET', 'Technical Skills', 'Sof Skills']\n",
            "Extracted Projects: ['Technical Content Writing', 'Space Exploration', 'Badminton', 'Swimming', 'INTEREST AND HOBBIES']\n",
            "\n",
            "Email: johnwilliams@gmail.com\n",
            "Work Experience: 2014-2019\n",
            "California\n",
            "GPA: 8.7\n",
            "Senior Data Scientist\n",
            "Tasks\n",
            "• Developed end-to-end machine learning prototypes and scaled them to run in production environments. Increased \n",
            "eficiency by 23%\n",
            "• Derived actionable insights from massive data sets with minimal support. \n",
            "• Provided input into the collection of new data sources and the refinement of existing onces to improve analysis and \n",
            "model development. \n",
            "Achievements\n",
            "• Applied data mining to analyze procurement processes resulting in savings of $420,000 a year. \n",
            "Machine Learning\n",
            "Data Visualization\n",
            "Data Mining\n",
            "Python\n",
            "Scala\n",
            "NLP\n",
            "Problem Solving\n",
            "Fast Learner\n",
            "Leadership\n",
            "Risk Analyisis\n",
            "Time Management\n",
            "Skills: ['ET', 'Technical Skills', 'Sof Skills']\n",
            "Projects: ['Technical Content Writing', 'Space Exploration', 'Badminton', 'Swimming', 'INTEREST AND HOBBIES']\n",
            "Score: 5\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 0 points\n",
            " Projects: 5 points\n",
            " Total: 5 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_paths = ['/content/resume12.pdf']\n",
        "job_description = \"\"\"\n",
        "Skills Required:\n",
        "Python, Machine Learning, Data Analysis, NLP\n",
        "\n",
        "Job Description:\n",
        "We are looking for a skilled data scientist with experience in machine learning and data analysis.\n",
        "\"\"\"\n",
        "\n",
        "sorted_resumes = process_resumes(resume_paths, job_description)\n",
        "\n",
        "for resume in sorted_resumes:\n",
        "    print(f\"\\nEmail: {resume['email']}\")\n",
        "    print(f\"Work Experience: {resume['work_experience']}\")\n",
        "    print(f\"Skills: {resume['skills']}\")\n",
        "    print(f\"Projects: {resume['projects']}\")\n",
        "    print(f\"Score: {resume['score']}\")\n",
        "    print(f\"\\nScoring Breakdown\")\n",
        "    print(f\" Work Experience: {resume['score_details']['experience']} points\")\n",
        "    print(f\" Skills: {resume['score_details']['skills']} points\")\n",
        "    print(f\" Projects: {resume['score_details']['projects']} points\")\n",
        "    print(f\" Total: {resume['score']} points\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83gsOphmNa6T",
        "outputId": "be5d9e6b-23a0-4b3e-9f44-a32030cc7078"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Skills: ['Python', 'Machine Learning', 'Data Analysis', 'NLP']\n",
            "\n",
            "Processing /content/resume12.pdf...\n",
            "Extracted Name: Data Science\n",
            "Extracted Email: juanjose.carin@gmail.com\n",
            "Extracted Phone: 650-336-4590\n",
            "Extracted Work Experience: \n",
            "Extracted Skills: ['Programming / Statistics', 'Big Data', 'Visualization', 'Others', 'Proficient:', 'R', 'Python', 'SQL', 'Hadoop', 'Hive', 'MrJob', 'Tableau', 'Git', 'AWS', 'Intermediate:', 'SPSS', 'SAS', 'Matlab', 'Spark', 'Storm', 'Bash', 'Basic:', 'EViews', 'Demetra+', 'D3.js', 'Gephi', 'Neo4j', 'QGIS', 'Experience', 'DATA SCIENCE', 'Jan. 2016 – Mar. 2016', 'Data Scientist', 'CONENTO', 'Madrid', 'Spain (working remotely)', '• Designed and implemented the ETL pipeline for a predictive model of traffic on the main roads in', 'eastern Spain (a project for the Spanish government).', '• Automated scripts in R to extract', 'transform', 'clean (incl. anomaly detection)', 'and load into MySQL', 'data from multiple data sources: road traffic sensors', 'accidents', 'road works', 'weather.', 'Jun. 2014 – Sep. 2014', 'Spain', '• Designed an experiment for Google Spain (conducted in October 2014) to measure the impact of', \"YouTube ads on the sales of a car manufacturer's dealer network.\", '• A matched-pair', 'cluster-randomized design', 'which involved selecting the test and control groups', 'from a sample of 50+ cities in Spain (where geo-targeted ads were possible) based on their sales-', 'wise similarity over time', 'using wavelets (and R).', 'MANAGEMENT – SALES (Electrical Eng.)', 'Feb. 2009 – Aug. 2013', 'Head of Sales', 'Spain & Portugal – Test &Measurement dept.', 'YOKOGAWA', '• Applied analysis of sales and market trends to decide the direction of the department.', '• Led a team of 7 people.', '2 of 2', 'Juan Jose Carin', 'Mountain View', 'CA 94041', '650-336-4590 | juanjose.carin@gmail.com', 'linkedin.com/in/juanjosecarin |', 'juanjocarin.github.io', '• Increased revenue by 6.3%', 'gross profit by 4.2%', 'and operating income by 146%', 'and achieved a 30%', 'ratio of new customers (3x growth)', 'by entering new markets and improving customer service and', 'training.', 'SALES (Electrical Eng. & Telecom.)', 'Apr. 2008 – Jan. 2009', 'Sales Engineer – Test & Measurement dept.', '• Promoted to head of sales after 5 months leading the sales team.', 'Sep. 2004 – Mar. 2008', 'Sales & Application Engineer', 'AYSCOM', '• Exceeded sales target every year from 2005 to 2007 (achieved 60% of the target in the first 3 months', 'of 2008).']\n",
            "Extracted Projects: ['See juanjocarin.github.io for additional information', '2016', 'SmartCam', 'Capstone', 'Python, OpenCV, TensorFlow, AWS (EC2, S3, DynamoDB)', 'A scalable cloud-based video monitoring system that features motion detection, face counting, and image recognition.', '2015', 'Implementation of the Shortest Path and PageRank algorithms with the Wikipedia graph dataset', 'Machine Learning at Scale', 'Hadoop MrJob, Python, AWS EC2, AWS S3', 'Using a graph dataset of almost half a million nodes.', '2015', 'Forest cover type prediction', 'Machine Learning', 'Python, Scikit-Learn, Matplotlib', 'A Kaggle competition: predictions of the predominant kind of tree cover, from strictly cartographic variables such as elevation', 'and soil type, using random forests, SVMs, kNNs, Naive Bayes, Gradient Descent, GMMs, …', '2015', 'Redefining the job search process', 'Storing and Retrieving Data', 'Hadoop HDFS, Hive, Spark, Python, AWS EC2, Tableau', 'A pipeline that combines data from Indeed API and the U.S. Census Bureau to select the best locations for data scientists', 'based on the number of job postings, housing cost, etc.', '2015', 'A fresh perspective on Citi Bike', 'Data Visualization and Communication', 'Tableau, SQLite', 'An interactive website to visualize NYC Citi Bike bicycle sharing service.', '2015', 'Investigating the effect of competition on the ability to solve arithmetic problems', 'Field Experiments', 'R', 'A randomized controlled trial in which 300+ participants were assigned to a control group or one of two test groups to', 'evaluate the effect of competition (being compared to no one or someone better or worse).', '2014', 'Prediction of customer churn for a mobile network carrier', 'Data Mining', 'SAS', 'Predictions from a sample of 45,000+ customers, using tree decisions, logistic regression, and neural networks.', '2014', 'Different models of Harmonized Index of Consumer Prices (HICP) in Spain', 'Time Series', 'SPSS, Demetra+', 'Forecasts based on exponential smoothing, ARIMA, and transfer function (using petrol price as independent variable) models.']\n",
            "\n",
            "Email: juanjose.carin@gmail.com\n",
            "Work Experience: \n",
            "Skills: ['Programming / Statistics', 'Big Data', 'Visualization', 'Others', 'Proficient:', 'R', 'Python', 'SQL', 'Hadoop', 'Hive', 'MrJob', 'Tableau', 'Git', 'AWS', 'Intermediate:', 'SPSS', 'SAS', 'Matlab', 'Spark', 'Storm', 'Bash', 'Basic:', 'EViews', 'Demetra+', 'D3.js', 'Gephi', 'Neo4j', 'QGIS', 'Experience', 'DATA SCIENCE', 'Jan. 2016 – Mar. 2016', 'Data Scientist', 'CONENTO', 'Madrid', 'Spain (working remotely)', '• Designed and implemented the ETL pipeline for a predictive model of traffic on the main roads in', 'eastern Spain (a project for the Spanish government).', '• Automated scripts in R to extract', 'transform', 'clean (incl. anomaly detection)', 'and load into MySQL', 'data from multiple data sources: road traffic sensors', 'accidents', 'road works', 'weather.', 'Jun. 2014 – Sep. 2014', 'Spain', '• Designed an experiment for Google Spain (conducted in October 2014) to measure the impact of', \"YouTube ads on the sales of a car manufacturer's dealer network.\", '• A matched-pair', 'cluster-randomized design', 'which involved selecting the test and control groups', 'from a sample of 50+ cities in Spain (where geo-targeted ads were possible) based on their sales-', 'wise similarity over time', 'using wavelets (and R).', 'MANAGEMENT – SALES (Electrical Eng.)', 'Feb. 2009 – Aug. 2013', 'Head of Sales', 'Spain & Portugal – Test &Measurement dept.', 'YOKOGAWA', '• Applied analysis of sales and market trends to decide the direction of the department.', '• Led a team of 7 people.', '2 of 2', 'Juan Jose Carin', 'Mountain View', 'CA 94041', '650-336-4590 | juanjose.carin@gmail.com', 'linkedin.com/in/juanjosecarin |', 'juanjocarin.github.io', '• Increased revenue by 6.3%', 'gross profit by 4.2%', 'and operating income by 146%', 'and achieved a 30%', 'ratio of new customers (3x growth)', 'by entering new markets and improving customer service and', 'training.', 'SALES (Electrical Eng. & Telecom.)', 'Apr. 2008 – Jan. 2009', 'Sales Engineer – Test & Measurement dept.', '• Promoted to head of sales after 5 months leading the sales team.', 'Sep. 2004 – Mar. 2008', 'Sales & Application Engineer', 'AYSCOM', '• Exceeded sales target every year from 2005 to 2007 (achieved 60% of the target in the first 3 months', 'of 2008).']\n",
            "Projects: ['See juanjocarin.github.io for additional information', '2016', 'SmartCam', 'Capstone', 'Python, OpenCV, TensorFlow, AWS (EC2, S3, DynamoDB)', 'A scalable cloud-based video monitoring system that features motion detection, face counting, and image recognition.', '2015', 'Implementation of the Shortest Path and PageRank algorithms with the Wikipedia graph dataset', 'Machine Learning at Scale', 'Hadoop MrJob, Python, AWS EC2, AWS S3', 'Using a graph dataset of almost half a million nodes.', '2015', 'Forest cover type prediction', 'Machine Learning', 'Python, Scikit-Learn, Matplotlib', 'A Kaggle competition: predictions of the predominant kind of tree cover, from strictly cartographic variables such as elevation', 'and soil type, using random forests, SVMs, kNNs, Naive Bayes, Gradient Descent, GMMs, …', '2015', 'Redefining the job search process', 'Storing and Retrieving Data', 'Hadoop HDFS, Hive, Spark, Python, AWS EC2, Tableau', 'A pipeline that combines data from Indeed API and the U.S. Census Bureau to select the best locations for data scientists', 'based on the number of job postings, housing cost, etc.', '2015', 'A fresh perspective on Citi Bike', 'Data Visualization and Communication', 'Tableau, SQLite', 'An interactive website to visualize NYC Citi Bike bicycle sharing service.', '2015', 'Investigating the effect of competition on the ability to solve arithmetic problems', 'Field Experiments', 'R', 'A randomized controlled trial in which 300+ participants were assigned to a control group or one of two test groups to', 'evaluate the effect of competition (being compared to no one or someone better or worse).', '2014', 'Prediction of customer churn for a mobile network carrier', 'Data Mining', 'SAS', 'Predictions from a sample of 45,000+ customers, using tree decisions, logistic regression, and neural networks.', '2014', 'Different models of Harmonized Index of Consumer Prices (HICP) in Spain', 'Time Series', 'SPSS, Demetra+', 'Forecasts based on exponential smoothing, ARIMA, and transfer function (using petrol price as independent variable) models.']\n",
            "Score: 46\n",
            "\n",
            "Scoring Breakdown\n",
            " Work Experience: 0 points\n",
            " Skills: 2 points\n",
            " Projects: 44 points\n",
            " Total: 46 points\n"
          ]
        }
      ]
    }
  ]
}